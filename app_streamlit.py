from __future__ import annotations

# --- watcher de archivos (evita recargas agresivas en dev) ---
import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "poll"  # o "none" si prefieres desactivar

# ================== IMPORTS BASE ==================
import hashlib
import json
import time
from datetime import datetime
from typing import Tuple
import altair as alt
import numpy as np
import pandas as pd
import streamlit as st

# ============== IMPORTS DE TU PIPELINE ==============
from pipeline_factors import build_factor_frame
from fundamentals import (
    download_fundamentals,
    build_vfq_scores_dynamic,          # (importado si luego lo usas)
    download_guardrails_batch,
    apply_quality_guardrails,          # (importado si luego lo usas)
)
from scoring import (
    blend_breakout_qvm,                # (importado si luego lo usas)
    build_momentum_proxy,              # (importado si luego lo usas)
)
from data_io import (
    run_fmp_screener,
    filter_universe,                   # (importado si luego lo usas)
    load_prices_panel,
    load_benchmark,
    DEFAULT_START,
    DEFAULT_END,
)
from pipeline import (
    apply_trend_filter,                # (importado si luego lo usas)
    enrich_with_breakout,              # (importado si luego lo usas)
    market_regime_on,                  # (importado si luego lo usas)
)
from backtests import backtest_many

# Opcional (growth-aware). No se usan a√∫n en la UI, pero los dejamos importables.
from factors_growth_aware import (
    compute_qvm_scores,                # (importado si luego lo usas)
    apply_megacap_rules,               # (importado si luego lo usas)
)

# ================== KEYS SNAPSHOT ==================
SNAP_KEY  = "vfq_snapshot"
SNAP_META = "vfq_snapshot_meta"

# ============== UTILS UNIVERSO & SNAPSHOT ==============
def _universe_fingerprint(df_universe: pd.DataFrame) -> str:
    """Firma determinista orden-agn√≥stica del universo por s√≠mbolos."""
    syms = (
        df_universe.get("symbol", pd.Series([], dtype=str))
        .dropna()
        .astype(str)
        .sort_values()
    )
    raw = ("|".join(syms.tolist())).encode("utf-8")
    return hashlib.md5(raw).hexdigest()

def compute_vfq_snapshot(uni_df: pd.DataFrame) -> pd.DataFrame:
    """
    Calcula todos los factores VFQ sobre el universo dado
    y reinyecta sector/market_cap desde el universo actual.
    """
    universe_syms = (
        uni_df["symbol"].dropna().astype(str).unique().tolist()
    )
    feats = build_factor_frame(universe_syms)
    feats = (
        feats.drop(columns=["sector", "market_cap"], errors="ignore")
        .merge(uni_df[["symbol", "sector", "market_cap"]], on="symbol", how="left")
    )
    return feats

# ------------------ CACHES ------------------
@st.cache_data(ttl=3600, show_spinner=False)
def _cached_run_fmp_screener(
    *,
    limit: int,
    mcap_min: float,
    volume_min: int,
    ipo_days: int,
    cache_key: str,
) -> pd.DataFrame:
    """
    1) Pide universo base a FMP con filtros b√°sicos.
    2) Normaliza columnas core (symbol, sector, market_cap).
    3) Aplica filtros post-request por volumen y antig√ºedad IPO.
    """
    df = run_fmp_screener(
        limit=limit,
        mcap_min=mcap_min,
        volume_min=volume_min,
        fetch_profiles=True,
        cache_key=cache_key,
        force=False,  # importante para respetar cache_key
    )
    if df is None:
        return pd.DataFrame(columns=["symbol", "sector", "market_cap"])

    df = df.copy()

    # market cap normalizada ‚Üí market_cap
    if "market_cap" not in df.columns:
        if "marketCap" in df.columns:
            df["market_cap"] = pd.to_numeric(df["marketCap"], errors="coerce")
        else:
            df["market_cap"] = np.nan

    # sector seguro
    if "sector" not in df.columns:
        df["sector"] = "Unknown"
    else:
        s = df["sector"].astype(str)
        s = s.replace({"": "Unknown"})
        s = s.where(~s.isna(), "Unknown")
        df["sector"] = s

    # volumen num√©rico
    if "volume" in df.columns:
        df["volume"] = pd.to_numeric(df["volume"], errors="coerce")

    # ipoDate a datetime
    if "ipoDate" in df.columns:
        df["ipoDate"] = pd.to_datetime(df["ipoDate"], errors="coerce", utc=True)
    else:
        df["ipoDate"] = pd.NaT

    # -------- filtros post-request --------
    df = df[df["market_cap"] >= float(mcap_min)]
    if "volume" in df.columns:
        df = df[df["volume"] >= float(volume_min)]
    if df["ipoDate"].notna().any():
        cutoff = pd.Timestamp.utcnow().normalize() - pd.Timedelta(days=int(ipo_days))
        df = df[df["ipoDate"] < cutoff]

    # columnas m√≠nimas
    core_cols = ["symbol", "sector", "market_cap"]
    if "symbol" not in df.columns:
        if "ticker" in df.columns:
            df["symbol"] = df["ticker"].astype(str)
        else:
            df["symbol"] = ""

    out = (
        df[core_cols]
        .dropna(subset=["symbol"])
        .reset_index(drop=True)
    )
    return out

@st.cache_data(show_spinner=False)
def _cached_vfq_snapshot(uni_df: pd.DataFrame, uni_sig: str) -> pd.DataFrame:
    """
    Cachea el snapshot VFQ completo ligado a la firma del universo.
    Cualquier cambio de `uni_sig` invalida este cache autom√°ticamente.
    """
    _ = uni_sig  # se usa solo para invalidar el cach√© cuando cambia
    return compute_vfq_snapshot(uni_df)

@st.cache_data(ttl=3600, show_spinner=False)
def _cached_download_guardrails(symbols: Tuple[str, ...], cache_key: str) -> pd.DataFrame:
    return download_guardrails_batch(list(symbols), cache_key=cache_key, force=False)

@st.cache_data(ttl=3600, show_spinner=False)
def _cached_download_fundamentals(
    symbols: Tuple[str, ...],
    cache_key: str,
    mc_pairs: Tuple[Tuple[str, float], ...] | None = None,
) -> pd.DataFrame:
    mc_map = dict(mc_pairs or ())
    return download_fundamentals(list(symbols), market_caps=mc_map, cache_key=cache_key, force=False)

@st.cache_data(ttl=3600, show_spinner=False)
def _cached_load_prices_panel(symbols, start, end, cache_key=""):
    return load_prices_panel(symbols, start, end, cache_key=cache_key, force=False)

@st.cache_data(ttl=3600, show_spinner=False)
def _cached_load_benchmark(bench, start, end):
    return load_benchmark(bench, start, end)

# ------------------ HELPERS FORMATO ------------------

# ---- Guardrails helpers (QVM) ----
COVERAGE_COLS = ["profit_hits","netdebt_ebitda","accruals_ta","asset_growth","share_issuance"]

def _build_guardrails_base_from_snapshot(snapshot: pd.DataFrame, uni: pd.DataFrame) -> pd.DataFrame:
    """Toma el snapshot VFQ y le injerta sector/mcap + coverage_count; sin filtros."""
    df = (
        snapshot.drop(columns=["sector", "market_cap"], errors="ignore")
        .merge(uni[["symbol","sector","market_cap"]], on="symbol", how="left")
    )
    # coverage_count = cu√°ntas m√©tricas clave existen (no NaN)
    df["coverage_count"] = (
        df[COVERAGE_COLS]
        .apply(pd.to_numeric, errors="coerce")
        .notna()
        .sum(axis=1)
        .astype(int)
    )
    return df

def _fmt_mcap(x):
    try:
        x = float(x)
        if x >= 1e12: return f"${x/1e12:.2f}T"
        if x >= 1e9:  return f"${x/1e9:.2f}B"
        if x >= 1e6:  return f"${x/1e6:.2f}M"
        return f"${x:,.0f}"
    except Exception:
        return ""

# ==================== CONFIG B√ÅSICO ====================
st.set_page_config(
    page_title="Sistema QVM",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded",
)

# CSS suave
st.markdown(
    """
<style>
.block-container { padding-top: 1.25rem; padding-bottom: 2rem; }
h1, h2, h3 { letter-spacing: .2px; }
hr { border: 0; border-top: 1px solid rgba(255,255,255,.08); margin: .6rem 0 1rem 0; }
[data-testid="stDataFrame"] tbody tr:hover { background: rgba(59,130,246,.08) !important; }
[data-testid="stCaptionContainer"] { opacity: .85; }
</style>
""",
    unsafe_allow_html=True,
)

# ==================== HEADER ====================
l, r = st.columns([0.85, 0.15])
with l:
    st.markdown("<h1 style='margin-bottom:0'>QVM Screener</h1>", unsafe_allow_html=True)
    st.caption("Momentum estructural + Breakout t√©cnico + Value/Quality (VFQ)")
with r:
    st.caption(datetime.now().strftime("Actualizado: %d %b %Y %H:%M"))
st.markdown("<hr/>", unsafe_allow_html=True)

# ==================== SIDEBAR ====================
with st.sidebar:
    st.markdown("### ‚öôÔ∏è Controles")
    preset = st.segmented_control(
        "Preset",
        options=["Laxo", "Balanceado", "Estricto"],
        default="Balanceado",
    )

    # ---- Universo & Screener ----
    with st.expander("Universo & Screener", expanded=True):
        limit = st.slider("L√≠mite del universo", 50, 1000, 300, 50)

        min_mcap = st.number_input(
            "MarketCap m√≠nimo (USD)", value=5e8, step=1e8, format="%.0f"
        )

        volume_min = st.number_input(
            "Volumen m√≠nimo diario", value=500_000, step=50_000, format="%.0f"
        )

        ipo_days = st.slider("Antig√ºedad IPO (d√≠as)", 90, 1500, 365, 30)

    # ---- Fundamentales & Guardrails ----
    with st.expander("Fundamentales & Guardrails", expanded=False):
        min_cov_guard = st.slider("Cobertura VFQ m√≠nima (# m√©tricas)", 1, 4, 2)
        profit_hits   = st.slider("Pisos de rentabilidad (hits EBIT/CFO/FCF)", 0, 3, 2)
        max_issuance  = st.slider("Net issuance m√°x.", 0.00, 0.10, 0.03, 0.01)
        max_assets    = st.slider("Asset growth |y/y| m√°x.", 0.00, 0.50, 0.20, 0.01)
        max_accr      = st.slider("Accruals/TA | | m√°x.", 0.00, 0.25, 0.10, 0.01)
        max_ndeb      = st.slider("NetDebt/EBITDA m√°x.", 0.0, 6.0, 3.0, 0.5)

    # ---- T√©cnico ----
    with st.expander("T√©cnico ‚Äî Tendencia & Breakout", expanded=True):
        use_and          = st.toggle("MA200 Y Mom 12‚Äì1", value=False)
        require_breakout = st.toggle("Exigir Breakout para ENTRY", value=False)
        rvol_th          = st.slider("RVOL (20d) m√≠n.", 0.8, 2.5, 1.2, 0.1)
        closepos_th      = st.slider("ClosePos m√≠n.", 0.0, 1.0, 0.60, 0.05)
        p52_th           = st.slider("Cercan√≠a 52W High", 0.80, 1.00, 0.95, 0.01)
        updown_vol_th    = st.slider("Up/Down Vol Ratio (20d)", 0.8, 3.0, 1.2, 0.1)
        min_hits_brk     = st.slider("M√≠nimo checks breakout (K de 4)", 1, 4, 3)
        atr_pct_min      = st.slider("ATR pct (6‚Äì12m) m√≠n.", 0.0, 1.0, 0.6, 0.05)
        use_rs_slope     = st.toggle("Exigir RS slope > 0 (MA20)", value=False)

    # ---- R√©gimen & Fechas ----
    with st.expander("R√©gimen & Fechas", expanded=False):
        bench   = st.selectbox("Benchmark", ["SPY", "QQQ", "^GSPC"], index=0)
        risk_on = st.toggle("Exigir mercado Risk-ON", value=True)
        start   = st.date_input("Inicio", value=pd.to_datetime(DEFAULT_START).date())
        end     = st.date_input("Fin",    value=pd.to_datetime(DEFAULT_END).date())

    # ---- Ranking avanzado ----
    with st.expander("Ranking avanzado", expanded=False):
        beta_prob   = st.slider("Sensibilidad probabilidad (Œ≤)", 1.0, 12.0, 6.0, 0.5)
        top_n_show  = st.slider("Top N a resaltar", 10, 100, 25, 5)

    st.markdown("---")
    run_btn = st.button("Ejecutar", use_container_width=True)

# Presets que ajustan umbrales t√©cnicos, sin pisar lo que ya moviste manualmente demasiado
if preset == "Laxo":
    rvol_th     = min(rvol_th, 1.0)
    closepos_th = min(closepos_th, 0.55)
    p52_th      = min(p52_th, 0.92)
    min_hits_brk = min(min_hits_brk, 2)
elif preset == "Estricto":
    rvol_th     = max(rvol_th, 1.5)
    closepos_th = max(closepos_th, 0.65)
    p52_th      = max(p52_th, 0.97)
    min_hits_brk = max(min_hits_brk, 3)

# Cache tag que depende de entradas clave del universo
cache_tag = f"{int(min_mcap)}_{ipo_days}_{limit}_{int(volume_min)}"

# Estado del pipeline
if "pipeline_ready" not in st.session_state:
    st.session_state["pipeline_ready"] = False

# ==================== TABS ====================
tab1, tab2, tab3, tab4, tab6, tab7, tab8 = st.tabs(
    ["Universo", "Guardrails", "VFQ", "Se√±ales", "Export", "Backtesting", "Tuning"]
)

# ==================== VFQ sidebar extra ====================
with st.sidebar:
    st.markdown("‚öôÔ∏è Fundamentos (VFQ)")

    value_metrics_opts   = ["inv_ev_ebitda", "fcf_yield"]
    quality_metrics_opts = ["gross_profitability", "roic", "roa", "netMargin"]

    sel_value = st.multiselect("M√©tricas Value", options=value_metrics_opts, default=["inv_ev_ebitda", "fcf_yield"])
    sel_quality = st.multiselect("M√©tricas Quality", options=quality_metrics_opts, default=["gross_profitability", "roic"])

    c1x, c2x = st.columns(2)
    with c1x:
        w_value = st.slider("Peso Value", 0.0, 1.0, 0.5, 0.05)
    with c2x:
        w_quality = st.slider("Peso Quality", 0.0, 1.0, 0.5, 0.05)

    method_intra = st.radio("Agregaci√≥n intra-bloque", ["mean", "median", "weighted_mean"], index=0, horizontal=True)
    winsor_p     = st.slider("Winsor p (cola)", 0.0, 0.10, 0.01, 0.005)
    size_buckets = st.slider("Buckets por tama√±o", 1, 5, 3, 1)
    group_mode   = st.selectbox("Agrupar por", ["sector", "sector|size"], index=1)
    min_cov      = st.slider("Cobertura m√≠n. (# m√©tricas)", 0, 8, 1, 1)
    min_pct      = st.slider("VFQ pct (intra-sector) m√≠n.", 0.00, 1.00, 0.00, 0.01)

    st.session_state["min_cov"] = int(min_cov)
    st.session_state["min_pct"] = float(min_pct)

vfq_cfg = dict(
    value_metrics=sel_value,
    quality_metrics=sel_quality,
    w_value=float(w_value),
    w_quality=float(w_quality),
    method_intra=method_intra,
    winsor_p=float(winsor_p),
    size_buckets=int(size_buckets),
    group_mode=group_mode,
)

# ====== TAB 1: UNIVERSO ======
with tab1:
    st.subheader("Universo inicial")

    # refrescamos universo si:
    #   - nunca se cre√≥
    #   - tocaste "Ejecutar"
    need_refresh = ("uni" not in st.session_state) or run_btn

    if need_refresh:
        raw_universe = _cached_run_fmp_screener(
            limit=limit,
            mcap_min=min_mcap,
            volume_min=volume_min,
            ipo_days=ipo_days,
            cache_key=cache_tag,
        )
        st.session_state["uni"] = raw_universe.copy()

    # leer versi√≥n estable en memoria
    uni_df = st.session_state["uni"].copy()

    total_raw = len(uni_df)
    total_filtrado = len(uni_df)  # ac√° podr√≠as hacer m√°s filtros si quieres

    c1m, c2m = st.columns(2)
    c1m.metric("Screener", f"{total_raw}")
    c2m.metric("Tras filtros b√°sicos", f"{total_filtrado}")

    st.dataframe(uni_df.head(50), hide_index=True, use_container_width=True)
    st.caption("Esta tabla vive en st.session_state['uni'] y alimenta las dem√°s pesta√±as.")

    # Firma del universo basada en s√≠mbolos (orden-agn√≥stica)
    st.session_state["uni_sig"] = _universe_fingerprint(uni_df)

    # (Opcional y recomendado) Par√°m. que definen el universo: usa tus controles REALES
    st.session_state["universe_norm_params"] = {
        "n_universe": int(limit),          # slider de tama√±o de universo real
        "winsor_p": float(winsor_p),       # slider VFQ
        "buckets": int(size_buckets),      # slider VFQ
        "group_by": group_mode,            # selector VFQ
    }

# ====== TAB 2: GUARDRAILS ======
# ====== TAB 2: GUARDRAILS ======
with tab2:
    st.subheader("Guardrails")

    uni = st.session_state.get("uni", pd.DataFrame())
    if uni is None or uni.empty or "symbol" not in uni.columns:
        st.info("Primero genera el universo en la pesta√±a Universo.")
        st.stop()

    # 1) Construimos / recuperamos la BASE (snapshot + coverage) una sola vez por universo
    uni_sig = st.session_state.get("uni_sig", "")
    need_rebuild = (
        ("qvm_guard_uni_sig" not in st.session_state) or
        ("qvm_guardrails_base" not in st.session_state) or
        (st.session_state["qvm_guard_uni_sig"] != uni_sig) or
        run_btn  # si apretaste Ejecutar, refrescamos
    )
    if need_rebuild:
        snapshot_vfq = _cached_vfq_snapshot(uni, uni_sig)  # <‚Äî usa tu cach√© ya definido
        base = _build_guardrails_base_from_snapshot(snapshot_vfq, uni)
        st.session_state["qvm_guardrails_base"] = base
        st.session_state["qvm_guard_uni_sig"] = uni_sig

    base = st.session_state["qvm_guardrails_base"].copy()

    # 2) Aplicamos SOLO filtros seg√∫n sliders (sin recalcular factores)
    # sliders ya definidos en sidebar: min_cov_guard, profit_hits, max_issuance, max_assets, max_accr, max_ndeb
    # (aseguramos tipos y NaNs)
    def _num(s, absval=False):
        s = pd.to_numeric(base[s], errors="coerce")
        return s.abs() if absval else s

    pass_profit   = (_num("profit_hits") >= int(profit_hits))
    pass_issuance = (_num("share_issuance", absval=True) <= float(max_issuance))
    pass_assets   = (_num("asset_growth", absval=True)   <= float(max_assets))
    pass_accruals = (_num("accruals_ta", absval=True)    <= float(max_accr))
    pass_ndebt    = (_num("netdebt_ebitda")              <= float(max_ndeb))
    pass_cover    = (pd.to_numeric(base["coverage_count"], errors="coerce") >= int(min_cov_guard))

    pass_all = pass_profit & pass_issuance & pass_assets & pass_accruals & pass_ndebt & pass_cover

    df_all = base.assign(
        pass_profit=pass_profit.fillna(False),
        pass_issuance=pass_issuance.fillna(False),
        pass_assets=pass_assets.fillna(False),
        pass_accruals=pass_accruals.fillna(False),
        pass_ndebt=pass_ndebt.fillna(False),
        pass_coverage=pass_cover.fillna(False),
        pass_all=pass_all.fillna(False),
    )

    kept_raw = df_all.loc[df_all["pass_all"], ["symbol"]].drop_duplicates().reset_index(drop=True)
    st.session_state["kept"] = kept_raw
    st.session_state["guard_diag"] = df_all.copy()

    total = len(df_all)
    pasan = int(df_all["pass_all"].sum())
    rechaz = total - pasan

    c1g, c2g, c3g = st.columns(3)
    c1g.metric("Pasan guardrails estrictos", f"{pasan}")
    c2g.metric("Candidatos saludables (relajado)", f"{pasan}")  # placeholder
    c3g.metric("Rechazados totales", f"{rechaz}")

    cols_show = [
        "symbol","sector","pass_all","profit_hits","coverage_count",
        "asset_growth","accruals_ta","netdebt_ebitda",
        "pass_profit","pass_issuance","pass_assets","pass_accruals","pass_ndebt","pass_coverage",
    ]
    cols_show = [c for c in cols_show if c in df_all.columns]

    with st.expander(f"Detalle guardrails (estricto): {pasan} / {total}", expanded=True):
        st.dataframe(
            df_all[cols_show].sort_values("symbol"),
            use_container_width=True,
            hide_index=True,
        )

    st.caption("pass_all = pas√≥ TODAS las barreras. coverage_count = cu√°nta info fundamental tenemos disponible.")

# ====== TAB 3: VFQ ======
with tab3:
    st.subheader("VFQ (Value / Quality / Flow)")

    kept   = st.session_state.get("kept", pd.DataFrame())
    uni_cur = st.session_state.get("uni", pd.DataFrame())

    if kept is None or kept.empty or "symbol" not in kept.columns:
        st.warning("No hay s√≠mbolos aprobados por Guardrails. Ajusta la pesta√±a Guardrails.")
        st.stop()

    kept_syms = kept["symbol"].dropna().astype(str).unique().tolist()
    if not kept_syms:
        st.warning("La lista kept est√° vac√≠a.")
        st.stop()

    # ------------------------------------------------------------
    # A) üîí SNAPSHOT VFQ FIJO con auto-invalidate por cambio de universo
    # ------------------------------------------------------------
    def _kept_signature(kept_syms: list[str], extra: dict | None = None) -> str:
        payload = {"kept": sorted(map(str, kept_syms))}
        if extra:
            payload["extra"] = extra
        raw = json.dumps(payload, sort_keys=True, separators=(",", ":")).encode()
        return hashlib.md5(raw).hexdigest()

    snap_key = SNAP_KEY
    meta_key = SNAP_META

    # Bot√≥n manual para forzar rec√°lculo
    recalc = st.button("‚ôªÔ∏è Recalcular snapshot VFQ (universo)", use_container_width=False)

    # 1) Firma compuesta del universo/kept
    universe_norm_params = st.session_state.get("universe_norm_params", {})
    uni_sig = st.session_state.get("uni_sig", "")
    cur_sig = _kept_signature(kept_syms, extra={"universe": universe_norm_params, "uni_sig": uni_sig})

    # 2) ¬øDebemos reconstruir?
    need_rebuild = recalc or (snap_key not in st.session_state)
    if not need_rebuild:
        prev_meta = st.session_state.get(meta_key, {})
        need_rebuild = (prev_meta.get("kept_sig") != cur_sig)

    if need_rebuild:
        # --- Construye snapshot nuevo con el UNIVERSO ACTUAL filtrado a kept ---
        uni_kept = uni_cur[uni_cur["symbol"].astype(str).isin(kept_syms)].copy()

        # üîê Cacheado por firma de universo
        df_vfq_all = _cached_vfq_snapshot(uni_kept, uni_sig)

        # Orden determinista (llaves ‚Äúcore‚Äù)
        df_vfq_all = df_vfq_all.assign(
            _p=df_vfq_all.get("prob_up", pd.Series(-9e9, index=df_vfq_all.index)).fillna(-9e9),
            _b=df_vfq_all.get("BreakoutScore", pd.Series(-9e9, index=df_vfq_all.index)).fillna(-9e9),
            _q=df_vfq_all.get("quality_adj_neut", pd.Series(-9e9, index=df_vfq_all.index)).fillna(-9e9),
            _v=df_vfq_all.get("value_adj_neut", pd.Series(-9e9, index=df_vfq_all.index)).fillna(-9e9),
        ).sort_values(
            ["_p", "_b", "_q", "_v", "symbol"],
            ascending=[False, False, False, False, True],
            kind="mergesort",
        )

        # Percentil por tama√±o (carril mega-cap)
        if "market_cap" in df_vfq_all.columns:
            df_vfq_all["cap_pct"] = df_vfq_all["market_cap"].rank(pct=True)

        st.session_state[snap_key] = df_vfq_all.copy()
        st.session_state[meta_key] = {
            "kept_sig": cur_sig,
            "n_kept": len(kept_syms),
            "ts": time.time(),
        }

    # 3) Consumimos SIEMPRE el snapshot fijo
    df_vfq_all = st.session_state[snap_key].copy()
    meta = st.session_state.get(meta_key, {})
    st.caption(
        f"Snapshot fijo: {meta.get('n_kept','?')} s√≠mbolos en kept. "
        f"(ts={meta.get('ts','‚Äî')}). Se actualiza solo si cambia el universo/kept o presionas el bot√≥n."
    )

    # -------------------------------------
    # B) üéõÔ∏è SLIDERS (+ toggle carril mega-cap)
    # -------------------------------------
    c1v, c2v, c3v = st.columns(3)
    with c1v:
        min_quality = st.slider("Min Quality neut.", 0.0, 1.0, 0.30, 0.01)
        min_value   = st.slider("Min Value neut.",   0.0, 1.0, 0.30, 0.01)
        max_ndebt   = st.slider("Max NetDebt/EBITDA", 0.0, 5.0, 3.0, 0.1)
    with c2v:
        min_acc_pct   = st.slider("Accruals limpios (% m√≠nimo)", 0, 100, 30, 1)
        min_hits_req  = st.slider("Min hits (breakout hits)",     0, 5,  2, 1)
        min_rvol20    = st.slider("Min RVOL20",                   0.0, 5.0, 1.50, 0.05)
    with c3v:
        min_breakout  = st.slider("Min BreakoutScore", 0, 100, 80, 1)
        topN_prob     = st.slider("Top N por prob_up", 5, 100, 30, 1)
        relax_mega    = st.toggle("‚öñÔ∏è Aflojar t√©cnica para mega-caps (top 10% cap)", value=True)

    # -----------------------------------------------------------------
    # C) üß™ Filtros sin re-normalizar + orden estable + carril mega-cap
    # -----------------------------------------------------------------
    is_mega = df_vfq_all.get("cap_pct", pd.Series(0, index=df_vfq_all.index)) >= 0.90  # top 10% por market cap

    # Reglas t√©cnicas ‚Äúsize-aware‚Äù (si activas el toggle)
    if relax_mega:
        hits_req = np.where(is_mega, np.maximum(1,  min_hits_req-1), min_hits_req)
        rvol_req = np.where(is_mega, np.maximum(1.1, min_rvol20-0.3), min_rvol20)
        brk_req  = np.where(is_mega, np.maximum(60,  min_breakout-10), min_breakout)
    else:
        hits_req = min_hits_req
        rvol_req = min_rvol20
        brk_req  = min_breakout

    m = pd.Series(True, index=df_vfq_all.index, dtype=bool)
    m &= df_vfq_all.get("quality_adj_neut", pd.Series(0, index=df_vfq_all.index)).fillna(0) >= float(min_quality)
    m &= df_vfq_all.get("value_adj_neut",   pd.Series(0, index=df_vfq_all.index)).fillna(0) >= float(min_value)
    m &= (df_vfq_all.get("acc_pct", pd.Series(np.nan, index=df_vfq_all.index)).isna()
          | (df_vfq_all.get("acc_pct").fillna(0) >= float(min_acc_pct)))
    m &= (df_vfq_all.get("netdebt_ebitda", pd.Series(np.nan, index=df_vfq_all.index)).isna()
          | (df_vfq_all.get("netdebt_ebitda").fillna(0) <= float(max_ndebt)))

    # T√©cnica (ya size-aware si relax_mega=True)
    m &= df_vfq_all.get("hits", pd.Series(0, index=df_vfq_all.index)).fillna(0)          >= hits_req
    m &= df_vfq_all.get("RVOL20", pd.Series(0, index=df_vfq_all.index)).fillna(0)        >= rvol_req
    m &= df_vfq_all.get("BreakoutScore", pd.Series(0, index=df_vfq_all.index)).fillna(0) >= brk_req

    df_keep_vfq = df_vfq_all.loc[m].copy()

    # Orden estable ya viene del snapshot; reforzamos por las llaves de prioridad
    df_keep_vfq = df_keep_vfq.sort_values(
        ["_p", "_b", "_q", "_v", "symbol"],
        ascending=[False, False, False, False, True],
        kind="mergesort",
    )

    vfq_top = df_keep_vfq.head(int(topN_prob)).copy()

    # --- Render tablas ---
    st.markdown("### üü¢ Selecci√≥n VFQ filtrada")
    cols_vfq_show = [
        "symbol", "netdebt_ebitda", "accruals_ta", "sector", "market_cap",
        "quality_adj_neut", "value_adj_neut", "acc_pct",
        "hits", "BreakoutScore", "RVOL20", "prob_up",
    ]
    cols_vfq_show = [c for c in cols_vfq_show if c in vfq_top.columns]
    st.dataframe(vfq_top[cols_vfq_show], use_container_width=True, hide_index=True)

    # Rechazados
    st.markdown("### üßπ Rechazados por VFQ / t√©cnica")
    rejected_syms = sorted(set(df_vfq_all["symbol"]) - set(df_keep_vfq["symbol"]))
    rej_view = df_vfq_all[df_vfq_all["symbol"].isin(rejected_syms)].copy()
    cols_rej_show = [
        "symbol", "sector", "market_cap", "quality_adj_neut", "value_adj_neut",
        "netdebt_ebitda", "acc_pct", "BreakoutScore", "hits", "RVOL20", "prob_up",
    ]
    cols_rej_show = [c for c in cols_rej_show if c in rej_view.columns]
    st.dataframe(rej_view[cols_rej_show], use_container_width=True, hide_index=True)

    # Guardar en session_state
    st.session_state["vfq_top"]      = vfq_top[["symbol"]].drop_duplicates()
    st.session_state["vfq_table"]    = vfq_top.reset_index(drop=True)
    st.session_state["vfq_all"]      = df_vfq_all.copy()   # snapshot entero (¬°fijo!)
    st.session_state["vfq_keep"]     = df_keep_vfq.copy()
    st.session_state["vfq_rejected"] = rej_view.copy()
    st.session_state["vfq_params"]   = {
        "min_hits": int(min_hits_req),
        "min_rvol20": float(min_rvol20),
        "min_breakout": float(min_breakout),
        "min_acc_pct": float(min_acc_pct),
        "max_ndebt": float(max_ndebt),
        "relax_mega": bool(relax_mega),
    }

    # ------------------------------------------------------------
    # D) üîé Inspector ‚Äú¬øpor qu√© no sale X?‚Äù (al final del tab)
    # ------------------------------------------------------------
    with st.expander("üîé ¬øPor qu√© no aparece un s√≠mbolo?"):
        q = st.text_input("Ticker", "GOOG").strip().upper()
        if q:
            row = df_vfq_all[df_vfq_all["symbol"] == q].head(1)
            if row.empty:
                st.info("No est√° en el universo kept del snapshot.")
            else:
                r = row.iloc[0]
                # t√©cnica size-aware consistente con el filtro
                _is_mega = bool(r.get("cap_pct", 0) >= 0.90)
                _hits_req = max(1, min_hits_req-1) if (relax_mega and _is_mega) else min_hits_req
                _rvol_req = max(1.1, min_rvol20-0.3) if (relax_mega and _is_mega) else min_rvol20
                _brk_req  = max(60,  min_breakout-10) if (relax_mega and _is_mega) else min_breakout

                checks = {
                    "quality_adj_neut": r.get("quality_adj_neut", 0) >= float(min_quality),
                    "value_adj_neut":   r.get("value_adj_neut", 0)   >= float(min_value),
                    "acc_pct":          (pd.isna(r.get("acc_pct")) or r.get("acc_pct") >= float(min_acc_pct)),
                    "netdebt_ebitda":   (pd.isna(r.get("netdebt_ebitda")) or r.get("netdebt_ebitda") <= float(max_ndebt)),
                    "hits":             r.get("hits", 0)            >= _hits_req,
                    "RVOL20":           r.get("RVOL20", 0)          >= _rvol_req,
                    "BreakoutScore":    r.get("BreakoutScore", 0)   >= _brk_req,
                }
                st.write({k: ("‚úÖ" if v else "‚ùå") for k, v in checks.items()})
                st.dataframe(row.T, use_container_width=True)


# ====== TAB 4: SE√ëALES (placeholder por ahora) ======
# ====== TAB 4: SE√ëALES ======
with tab4:
    st.subheader("Se√±ales t√©cnicas / Breakout")

    # -------------------------
    # Recuperos del Tab 3
    # -------------------------
    uni_df     = st.session_state.get("uni", pd.DataFrame())
    vfq_all    = st.session_state.get("vfq_all", pd.DataFrame())
    vfq_keep   = st.session_state.get("vfq_keep", pd.DataFrame())
    vfq_rej    = st.session_state.get("vfq_rejected", pd.DataFrame())
    vfq_params = st.session_state.get("vfq_params", {})

    if vfq_all is None or vfq_all.empty:
        st.info("Todav√≠a no hay datos t√©cnicos / VFQ. Corre la pesta√±a VFQ primero.")
        st.stop()

    # Umbrales desde VFQ con fallbacks seguros
    min_hits_thr       = int(vfq_params.get("min_hits", 1))
    min_breakout_thr   = float(vfq_params.get("min_breakout", 50.0))
    min_rvol20_thr     = float(vfq_params.get("min_rvol20", 1.2))
    require_breakout_flag = bool(vfq_params.get("require_breakout", False))

    # Risk-ON: intenta leer de √°mbito local; si no, de session_state; si no, False
    try:
        global_risk_on = bool(risk_on)  # del sidebar (scope superior)
    except NameError:
        global_risk_on = bool(st.session_state.get("_risk_on", False))

    # -------------------------
    # 1) Estado de mercado
    # -------------------------
    st.markdown("### 1. Estado de mercado")

    hits_ser = pd.to_numeric(vfq_all.get("hits", pd.Series(dtype=float)), errors="coerce").fillna(0)
    bks_ser  = pd.to_numeric(vfq_all.get("BreakoutScore", pd.Series(dtype=float)), errors="coerce").fillna(0)

    pct_hits_ok = hits_ser.ge(min_hits_thr).mean() if len(hits_ser) else np.nan
    pct_breakout_ok = bks_ser.ge(min_breakout_thr).mean() if len(bks_ser) else np.nan

    c1, c2, c3 = st.columns(3)
    c1.metric("% setups t√©cnicos OK", f"{pct_hits_ok*100:.1f}%" if not np.isnan(pct_hits_ok) else "n/d",
              help="‚âà % del universo con suficientes 'hits' (checks t√©cnicos cumplidos).")
    c2.metric("% ruptura/momentum OK", f"{pct_breakout_ok*100:.1f}%" if not np.isnan(pct_breakout_ok) else "n/d",
              help="‚âà % del universo con BreakoutScore ‚â• umbral.")
    c3.metric("R√©gimen mercado", "RISK ON ‚úÖ" if global_risk_on else "RISK OFF ‚ö†Ô∏è",
              help="Switch macro/t√°ctico desde la barra lateral.")

    st.caption(
        "- % setups t√©cnicos OK ‚âà amplitud por 'hits'.  \n"
        "- % ruptura/momentum OK ‚âà cu√°ntas rompen con fuerza (BreakoutScore).  \n"
        "- Con RISK ON + amplitud alta ‚Üí mejor clima para entradas nuevas."
    )
    st.markdown("---")

    # -------------------------
    # 2) Checklist t√©cnico activo
    # -------------------------
    st.markdown("### 2. Checklist t√©cnico activo")
    col_a, col_b, col_c, col_d = st.columns(4)
    col_a.metric("Hits m√≠nimos", f"{min_hits_thr}")
    col_b.metric("BreakoutScore m√≠n.", f"{min_breakout_thr:.1f}")
    col_c.metric("RVOL20 m√≠n.", f"{min_rvol20_thr:.2f}")
    col_d.metric("¬øRequiere breakout?", "S√≠" if require_breakout_flag else "No")

    st.caption("Par√°metros heredados de VFQ/t√©cnico (tab 3).")
    st.markdown("---")

    # -------------------------
    # 3) Watchlist t√©cnica
    # -------------------------
    st.markdown("### 3. Watchlist t√©cnica (post-VFQ + t√©cnico)")
    if vfq_keep is None or vfq_keep.empty:
        st.warning("Ning√∫n ticker pas√≥ VFQ + t√©cnico con los filtros actuales.")
    else:
        cols_keep_show = [
            "symbol","sector","market_cap","quality_adj_neut","value_adj_neut",
            "acc_pct","hits","BreakoutScore","RVOL20","prob_up",
        ]
        cols_keep_show = [c for c in cols_keep_show if c in vfq_keep.columns]
        st.dataframe(
            vfq_keep[cols_keep_show].sort_values(
                ["prob_up","BreakoutScore"], ascending=[False, False], na_position="last"
            ),
            hide_index=True, use_container_width=True
        )

    st.caption(
        "Candidatos que pasaron Guardrails + VFQ y cumplen se√±ales de tendencia/momentum/volumen."
    )
    st.markdown("---")

    # -------------------------
    # 4) Rechazados t√©cnicos
    # -------------------------
    st.markdown("### 4. Rechazados t√©cnicos")
    if vfq_rej is None or vfq_rej.empty:
        st.info("No hay rechazados t√©cnicos adicionales (o no se guardaron).")
    else:
        cols_rej_show = [
            "symbol","sector","market_cap","quality_adj_neut","value_adj_neut",
            "netdebt_ebitda","acc_pct","hits","BreakoutScore","RVOL20","prob_up",
        ]
        cols_rej_show = [c for c in cols_rej_show if c in vfq_rej.columns]
        st.dataframe(
            vfq_rej[cols_rej_show].sort_values(
                ["BreakoutScore","prob_up"], ascending=[False, False], na_position="last"
            ),
            hide_index=True, use_container_width=True
        )

    # -------------------------
    # 5) Nota final
    # -------------------------
    st.info(
        "Gu√≠a r√°pida:\n"
        "- Amplitud alta + RISK ON ‚úÖ ‚Üí entorno favorable.\n"
        "- Amplitud baja o RISK OFF ‚ö†Ô∏è ‚Üí reduce agresividad/tama√±o."
    )


# ====== TAB 6: EXPORT (placeholder) ======
with tab6:
    st.subheader("Export")
    st.caption("Descarga tus vistas actuales.")
    cex1, cex2, cex3 = st.columns(3)

    # Versiones actuales en memoria
    v_uni  = st.session_state.get("uni", pd.DataFrame())
    v_all  = st.session_state.get("vfq_all", pd.DataFrame())
    v_keep = st.session_state.get("vfq_keep", pd.DataFrame())
    v_rej  = st.session_state.get("vfq_rejected", pd.DataFrame())

    def _csv_bytes(df: pd.DataFrame) -> bytes:
        return df.to_csv(index=False).encode("utf-8") if isinstance(df, pd.DataFrame) and not df.empty else b""

    with cex1:
        st.download_button("‚¨áÔ∏è Universo (uni)", data=_csv_bytes(v_uni), file_name="universe.csv", mime="text/csv", use_container_width=True)
    with cex2:
        st.download_button("‚¨áÔ∏è Snapshot VFQ (all)", data=_csv_bytes(v_all), file_name="vfq_all.csv", mime="text/csv", use_container_width=True)
    with cex3:
        st.download_button("‚¨áÔ∏è Selecci√≥n VFQ (keep)", data=_csv_bytes(v_keep), file_name="vfq_keep.csv", mime="text/csv", use_container_width=True)

    st.download_button("‚¨áÔ∏è Rechazados VFQ/t√©cnico", data=_csv_bytes(v_rej), file_name="vfq_rejected.csv", mime="text/csv", use_container_width=True)


# ====== TAB 7: BACKTESTING ======
with tab7:
    st.subheader("Backtesting")

    vfq_keep = st.session_state.get("vfq_keep", pd.DataFrame())
    if vfq_keep is None or vfq_keep.empty or "symbol" not in vfq_keep.columns:
        st.warning("No hay s√≠mbolos aprobados en VFQ + t√©cnico. Corre las pesta√±as anteriores primero.")
        st.stop()

    all_syms_bt = vfq_keep["symbol"].dropna().astype(str).unique().tolist()
    if not all_syms_bt:
        st.warning("No hay s√≠mbolos v√°lidos para backtest.")
        st.stop()

    # ---------- Controles ----------
    st.markdown("#### Par√°metros de simulaci√≥n")
    c_bt1, c_bt2, c_bt3 = st.columns(3)
    with c_bt1:
        top_n_bt = st.slider("N¬∞ m√°x. de s√≠mbolos a probar", min_value=5, max_value=min(50, len(all_syms_bt)),
                             value=min(20, len(all_syms_bt)), step=1)
    with c_bt2:
        cost_bps = st.number_input("Costo de trading (bps por cambio de postura)", min_value=0, max_value=100,
                                   value=10, step=1)
        lag_days = st.number_input("Lag ejecuci√≥n (d√≠as)", min_value=0, max_value=5, value=0, step=1)
    with c_bt3:
        use_and_condition = st.toggle("Exigir MA200 Y Mom12-1 > 0", value=False)
        rebalance_freq = st.selectbox("Frecuencia rebalance", options=["M", "W"], index=0)

    syms_bt = all_syms_bt[: int(top_n_bt)]
    st.caption(
        f"Testeando {len(syms_bt)} s√≠mbolos: {', '.join(syms_bt[:10])}" + ("‚Ä¶" if len(syms_bt) > 10 else "")
    )

    # ---------- Fechas ----------
    try:
        start_dt = pd.to_datetime(start).date()
        end_dt   = pd.to_datetime(end).date()
    except NameError:
        start_dt = pd.to_datetime(DEFAULT_START).date()
        end_dt   = pd.to_datetime(DEFAULT_END).date()

    # ---------- Precios ----------
    price_panel = _cached_load_prices_panel(
        symbols=syms_bt,
        start=start_dt,
        end=end_dt,
        cache_key=f"bt_{start_dt}_{end_dt}_{len(syms_bt)}"
    )
    if not isinstance(price_panel, dict) or not price_panel:
        st.error("No pude cargar precios hist√≥ricos para backtest.")
        st.stop()

    # ---------- Backtest ----------
    bt_metrics, bt_curves = backtest_many(  # usa el m√≥dulo ya importado arriba
        panel=price_panel,
        symbols=syms_bt,
        cost_bps=int(cost_bps),
        lag_days=int(lag_days),
        use_and_condition=bool(use_and_condition),
        rebalance_freq=str(rebalance_freq)
    )

    # ---------- M√©tricas ----------
    st.markdown("#### Resultados por s√≠mbolo")
    if bt_metrics is None or bt_metrics.empty:
        st.warning("No hubo data suficiente para calcular m√©tricas.")
    else:
        show_cols = [c for c in ["symbol","CAGR","Sharpe","Sortino","MaxDD","Turnover","Trades"] if c in bt_metrics.columns]
        fmt_df = bt_metrics.copy()
        if "CAGR" in fmt_df:  fmt_df["CAGR"]   = (fmt_df["CAGR"] * 100).round(2)
        if "MaxDD" in fmt_df: fmt_df["MaxDD"]  = (fmt_df["MaxDD"] * 100).round(2)
        if "Turnover" in fmt_df: fmt_df["Turnover"] = (fmt_df["Turnover"] * 100).round(2)

        st.dataframe(fmt_df[show_cols], hide_index=True, use_container_width=True)
        st.caption(
            "- CAGR anualizada; MaxDD y Turnover en %.  \n"
            "- Se√±al simple long-only binaria por ticker."
        )

    st.markdown("---")

    # ---------- Curvas (Altair) ----------
    st.markdown("#### Curvas de equity normalizadas (1.0 = inicio)")
    if not bt_curves:
        st.info("No hay curvas de equity para graficar.")
    else:
        eq_df_list = []
        for sym, curve in bt_curves.items():
            if curve is None or curve.empty:
                continue
            tmp = curve.rename("equity").to_frame().reset_index().rename(columns={"index": "date"})
            tmp["symbol"] = sym
            eq_df_list.append(tmp)

        if not eq_df_list:
            st.info("No se pudo armar data suficiente para el gr√°fico.")
        else:
            long_eq = pd.concat(eq_df_list, ignore_index=True).sort_values(["symbol","date"])

            def _norm_grp(g):
                first_val = g["equity"].iloc[0] if len(g) else np.nan
                g["equity_norm"] = g["equity"] / first_val if (pd.notna(first_val) and first_val != 0) else np.nan
                return g

            long_eq = long_eq.groupby("symbol", group_keys=False).apply(_norm_grp)

            chart = (
                alt.Chart(long_eq)
                .mark_line()
                .encode(
                    x=alt.X("date:T", title="Fecha"),
                    y=alt.Y("equity_norm:Q", title="Equidad normalizada"),
                    color=alt.Color("symbol:N", title="S√≠mbolo"),
                    tooltip=[alt.Tooltip("date:T", title="Fecha"),
                             alt.Tooltip("symbol:N", title="Ticker"),
                             alt.Tooltip("equity_norm:Q", title="Equidad norm.", format=".2f")]
                )
                .properties(height=320)
                .interactive()
            )
            st.altair_chart(chart, use_container_width=True)
            st.caption("Cada l√≠nea: dentro cuando se√±al ON, cash cuando OFF; reinvertido.")

# ====== TAB 8: TUNING (Random Search) ======
with tab8:
    import numpy as np
    import pandas as pd

    st.subheader("üîß Tuning de umbrales (random search)")

    kept       = st.session_state.get("kept", pd.DataFrame())
    df_vfq_all = st.session_state.get("vfq_all", pd.DataFrame())

    if kept is None or kept.empty or df_vfq_all is None or df_vfq_all.empty:
        st.warning("Necesitas correr Guardrails y VFQ antes de tunear.")
        st.stop()

    # Asegura 'acc_pct' si falta (a partir de 'accruals_ta')
    if "acc_pct" not in df_vfq_all.columns and "accruals_ta" in df_vfq_all.columns:
        s = pd.to_numeric(df_vfq_all["accruals_ta"], errors="coerce").astype(float)
        pct = (s.abs().rank(pct=True, method="average"))
        df_vfq_all["acc_pct"] = (1.0 - pct) * 100.0

    # --------- Par√°metros de b√∫squeda ----------
    c1, c2, c3 = st.columns(3)
    with c1:
        n_samples = st.number_input("N¬∞ combinaciones aleatorias", 20, 2000, 150, 10)
        cost_bps  = st.number_input("Costos (bps por rebalance)", 0, 100, 10, 1)
        use_and   = st.toggle("Tendencia: MA200 Y Mom12-1>0", value=False)
    with c2:
        try:
            start_tune = st.date_input("Inicio tuning", value=pd.to_datetime(start).date())
            end_tune   = st.date_input("Fin tuning", value=pd.to_datetime(end).date())
        except NameError:
            start_tune = st.date_input("Inicio tuning", value=pd.to_datetime(DEFAULT_START).date())
            end_tune   = st.date_input("Fin tuning", value=pd.to_datetime(DEFAULT_END).date())
        min_names  = st.number_input("M√≠n. s√≠mbolos por cartera", 5, 200, 15, 1)
    with c3:
        seed = st.number_input("Semilla aleatoria", 0, 10_000, 1234, 1)
        reb_freq = st.selectbox("Frecuencia rebalanceo", ["M","W","Q"], index=0)
        go_btn = st.button("Ejecutar Tuning", use_container_width=True, type="primary")

    ranges = dict(
        min_quality=(0.30, 0.70),
        min_value=(0.30, 0.70),
        min_acc_pct=(40, 85),           # %
        max_ndebt=(1.5, 3.0),
        min_hits_req=(0, 5),            # entero
        min_breakout=(50, 95),
        min_rvol20=(1.00, 2.50),
        topN_prob=(10, 60),
    )

    def _sample_params(rng: np.random.RandomState) -> dict:
        p = {
            "min_quality":  float(np.round(rng.uniform(*ranges["min_quality"]), 2)),
            "min_value":    float(np.round(rng.uniform(*ranges["min_value"]), 2)),
            "min_acc_pct":  int(rng.randint(*ranges["min_acc_pct"])),
            "max_ndebt":    float(np.round(rng.uniform(*ranges["max_ndebt"]), 1)),
            "min_breakout": int(rng.randint(*ranges["min_breakout"])),
            "min_rvol20":   float(np.round(rng.uniform(*ranges["min_rvol20"]), 2)),
            "min_hits_req": int(rng.randint(*ranges["min_hits_req"])),
            "topN_prob":    int(rng.randint(*ranges["topN_prob"])),
        }
        p["topN_prob"] = max(int(min_names), p["topN_prob"])
        return p

    def _rank_and_pick(df: pd.DataFrame, p: dict) -> list[str]:
        m = pd.Series(True, index=df.index, dtype=bool)
        m &= df.get("quality_adj_neut", pd.Series(0, index=df.index)).fillna(0) >= p["min_quality"]
        m &= df.get("value_adj_neut",   pd.Series(0, index=df.index)).fillna(0) >= p["min_value"]
        m &= df.get("hits", pd.Series(0, index=df.index)).fillna(0)             >= p["min_hits_req"]
        m &= df.get("BreakoutScore", pd.Series(0, index=df.index)).fillna(0)    >= p["min_breakout"]
        m &= df.get("RVOL20", pd.Series(0, index=df.index)).fillna(0)           >= p["min_rvol20"]
        m &= (df.get("acc_pct", pd.Series(np.nan, index=df.index)).isna()
              | (df.get("acc_pct", pd.Series(0, index=df.index)).fillna(0) >= p["min_acc_pct"]))
        m &= (df.get("netdebt_ebitda", pd.Series(np.nan, index=df.index)).isna()
              | (df.get("netdebt_ebitda", pd.Series(0, index=df.index)).fillna(0) <= p["max_ndebt"]))

        df_f = df.loc[m].copy()
        if df_f.empty:
            return []
        rank_col = "prob_up" if ("prob_up" in df_f.columns and df_f["prob_up"].notna().any()) else "BreakoutScore"
        df_f = df_f.sort_values(rank_col, ascending=False)
        return df_f["symbol"].dropna().astype(str).unique().tolist()[: int(p["topN_prob"])]

    def _portfolio_metrics_from_curves(curves: dict[str, pd.Series], freq_code: str) -> dict:
        if not curves:
            return {"CAGR":0,"Sharpe":0,"Sortino":0,"MaxDD":0,"N":0,"Turnover":0}
        eq = pd.DataFrame(curves).dropna(how="all")
        if eq.empty:
            return {"CAGR":0,"Sharpe":0,"Sortino":0,"MaxDD":0,"N":0,"Turnover":0}
        rets = eq.pct_change().mean(axis=1).fillna(0.0)
        periods = {"M":12,"W":52,"Q":4}[freq_code]

        # m√©tricas r√°pidas:
        mu = rets.mean() * periods
        sd = rets.std(ddof=0) * np.sqrt(periods)
        sharpe = float(mu / sd) if sd else 0.0

        dn = rets[rets < 0]
        sdd = dn.std(ddof=0) * np.sqrt(periods)
        sortino = float(mu / sdd) if sdd else 0.0

        eq_curve = (1 + rets).cumprod()
        years = len(rets) / float(periods) if periods else 0
        cagr = float(eq_curve.iloc[-1] ** (1.0/years) - 1.0) if (years > 0 and eq_curve.iloc[-1] > 0) else 0.0
        dd = (eq_curve / eq_curve.cummax() - 1.0).min()
        maxdd = float(dd) if pd.notna(dd) else 0.0

        return {"CAGR":cagr,"Sharpe":sharpe,"Sortino":sortino,"MaxDD":maxdd,"N":int(eq.shape[1])}

    results, details = [], []

    if go_btn:
        try:
            rng = np.random.RandomState(int(seed))
            pbar = st.progress(0.0, text="Buscando combinaciones‚Ä¶")

            for i in range(int(n_samples)):
                p = _sample_params(rng)
                picks = _rank_and_pick(df_vfq_all, p)
                pbar.progress((i+1)/float(n_samples), text=f"Eval {i+1}/{n_samples}")

                if len(picks) < int(min_names):
                    continue

                panel = _cached_load_prices_panel(
                    symbols=picks,
                    start=pd.to_datetime(start_tune).date(),
                    end=pd.to_datetime(end_tune).date(),
                    cache_key=f"tune_{len(picks)}_{start_tune}_{end_tune}"
                )
                if not isinstance(panel, dict) or not panel:
                    continue

                # Backtest (usa tu funci√≥n est√°ndar)
                metrics_df, curves = backtest_many(
                    panel=panel,
                    symbols=list(panel.keys()),
                    cost_bps=int(cost_bps),
                    lag_days=0,
                    use_and_condition=bool(use_and),
                    rebalance_freq=str(reb_freq),
                )
                avg_turn = float(metrics_df["Turnover"].mean()) if isinstance(metrics_df, pd.DataFrame) and not metrics_df.empty else 0.0

                port_perf = _portfolio_metrics_from_curves(curves, str(reb_freq))
                row = dict(
                    Sharpe=float(port_perf.get("Sharpe",0.0)),
                    Sortino=float(port_perf.get("Sortino",0.0)),
                    CAGR=float(port_perf.get("CAGR",0.0)),
                    MaxDD=float(port_perf.get("MaxDD",0.0)),
                    N=int(port_perf.get("N",0)),
                    Turnover=avg_turn,
                )
                row.update(p)
                results.append(row)
                details.append({"params": p, "picks": picks})

            pbar.empty()

        except Exception as e:
            st.error("Error durante el tuning.")
            st.exception(e)

    if results:
        res_df = pd.DataFrame(results).sort_values(["Sharpe","CAGR"], ascending=False).reset_index(drop=True)
        st.markdown("### üèÅ Top combinaciones")
        st.dataframe(res_df.head(25), use_container_width=True, hide_index=True)

        st.markdown("#### üìå Detalle de selecci√≥n")
        idx = st.number_input("Fila (Top-k) a inspeccionar", 0, max(0, len(res_df)-1), 0, 1)
        chosen = res_df.iloc[int(idx)].to_dict()
        st.json({k: chosen[k] for k in [
            "Sharpe","CAGR","Sortino","MaxDD","Turnover","N",
            "min_quality","min_value","min_acc_pct","max_ndebt",
            "min_hits_req","min_breakout","min_rvol20","topN_prob"
        ]})

        picks = details[int(idx)]["picks"]
        st.caption(f"Tickers ({len(picks)}): " + ", ".join(sorted(picks[:120])) + (" ‚Ä¶" if len(picks)>120 else ""))

        if st.button("üëâ Adoptar este preset", use_container_width=True):
            st.session_state["vfq_best_preset"] = {
                "from_tuning": True,
                "rebalance": str(reb_freq),
                "use_and": bool(use_and),
                "cost_bps": int(cost_bps),
                "date_range": (str(start_tune), str(end_tune)),
                "params": {k: chosen[k] for k in [
                    "min_quality","min_value","min_acc_pct","max_ndebt",
                    "min_hits_req","min_breakout","min_rvol20","topN_prob"
                ]},
                "metrics": {k: chosen[k] for k in ["Sharpe","CAGR","Sortino","MaxDD","Turnover","N"]},
                "picks": picks,
            }
            st.success("Preset guardado en st.session_state['vfq_best_preset']. Copia estos valores a los sliders de VFQ.")
